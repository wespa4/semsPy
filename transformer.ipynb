{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNJKsZM_w11r"
      },
      "source": [
        " # Реализация визуального трансформера\n",
        "\n",
        " Напишем свой визуальный трансформер для бинарной классификации картинок с кошками и собаками. Попробуем реализовать модель, которая принимает на вход картинку с произвольными размерами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0Tj_XkT3c_Zb",
        "outputId": "61f8771d-a528-4691-a5fe-9c5437d838bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koL6hJWtxJ8U"
      },
      "source": [
        "Скачиваем [датасет](https://www.kaggle.com/datasets/tongpython/cat-and-dog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "x52e0F4wdPxv"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tongpython/cat-and-dog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCM6lZIXxm11"
      },
      "source": [
        "## 1. Построение модели\n",
        "\n",
        "Изображение сначала нужно разбить на токены. Каждый токен - это часть изображения, представленная в виде ембеддинга.\n",
        "\n",
        "Напишем Токенизатор для разбиения изображения на токены. В нем мы используем сверточный слой, размер ядра которого равен размеру патча (фрагмента) картинки, а количество выходных каналов - размеру эмбединга. Предполагается, что размеры картинки нацело делятся на размер патча. В ембедингах не будет информации о позиции патча."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z6h267rnc_Zd"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(nn.Module):\n",
        "\t\"\"\"\n",
        "  Класс для разбиения картинки на токены\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, embed_size: int, patch_size: int):\n",
        "\t\t\"\"\"\n",
        "\t\tembed_size - размер эмбеддинга\n",
        "\t\tpatch_size - размер патча в пикселях\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.embed_size = embed_size\n",
        "\t\tself.patch_size = patch_size\n",
        "\n",
        "\t\tself.conv = nn.Conv2d(\n",
        "\t\t\tin_channels=3,\n",
        "\t\t\tout_channels=self.embed_size,\n",
        "\t\t\tkernel_size=self.patch_size,\n",
        "\t\t\tstride=self.patch_size\n",
        "\t\t)\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tРазбивает картинку на токены\n",
        "\n",
        "\t\tx - тензор картинок (тоже в виде тензора) размерностью (n_samples, n_channels, h_pixels, w_pixels)\n",
        "\n",
        "\t\tвозвращает тензор размера (n_samples, h_patches, w_patches, embedding_size)\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tx = self.conv(x).transpose(1,3).transpose(2,1)\n",
        "\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aBt1ExE2eu7"
      },
      "source": [
        "Далее нужно добавить к эмбеддингам информацию о положении патча на картинке. Для этого возьмем `Positional Encoding Generator` из статьи [Conditional Positional Encodings for Vision Transformers](https://arxiv.org/pdf/2102.10882v2). Этот способ кодировки позволяет модели обрабатывать картинки произвольного размера. В нем кодировка хранит информацию о ближайших соседях токена."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7aOwzY7Uc_Ze"
      },
      "outputs": [],
      "source": [
        "class PositionalEncodingGenerator(nn.Module):\n",
        "\t\"\"\"\n",
        "\tДвумерный positional encoding из статьи Conditional Positional Encodings for Vision Transformers\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, embed_size: int, kernel_size: int):\n",
        "\t\t\"\"\"\n",
        "\t\tembed_size - размер эмбединга токена\n",
        "\t\tkernel_size - размер куба, в котором рассматриваются ближайшие соседи, должен быть нечетным\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tif kernel_size % 2 == 0:\n",
        "\t\t\traise ValueError(\"kernel_size должен быть нечетным!\")\n",
        "\n",
        "\t\tself.conv = nn.Conv2d(\n",
        "\t\t\tin_channels=embed_size,\n",
        "\t\t\tout_channels=embed_size,\n",
        "\t\t\tkernel_size=kernel_size,\n",
        "\t\t\tstride=1,\n",
        "\t\t\tpadding = (kernel_size - 1) // 2\n",
        "\t\t)\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tДобавляет к эмбеддингам патчей информацию об их положении\n",
        "\n",
        "\t\tx - тензор эмбеддингов патчей размерностью (n_samples, h_patches, w_patches, embedding_size)\n",
        "\n",
        "\t\tвозвращает новый тензор с эмбеддингами того же размера\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# считаем позиционные ембеддинги\n",
        "\t\tpos_x = x.transpose(2,1).transpose(1,3)\n",
        "\t\tpos_x = self.conv(pos_x).transpose(1,3).transpose(2,1)\n",
        "\n",
        "\t\t# складываем позиционные ембеддинги с эмбеддингами токенов\n",
        "\t\treturn x + pos_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hauJ-tzc6dkS"
      },
      "source": [
        "Мы преобразовали картинку в токены и теперь можем приступать к написанию основной части трансформара. Сначала реализуем механизм `multi-head self attention`, в котором эмбеддинги насыщаются нформацией о связях с другими эмбеддингами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DjAyXfFac_Zf"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\t\"\"\"\n",
        "\tКласс реализации  multi-head self attention\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, emb_size: int, heads: int):\n",
        "\t\t\"\"\"\n",
        "\t\temb_size - размер эмбеддингов\n",
        "\t\theads - количество голов (видов связей между токенами)\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.k, self.heads = emb_size, heads\n",
        "\n",
        "\t\t# преобразования каждого эмбеддинга в heads векторов Q, K и V\n",
        "\t\tself.tokeys    = nn.Linear(emb_size, emb_size * heads, bias=False)  # Wk\n",
        "\t\tself.toqueries = nn.Linear(emb_size, emb_size * heads, bias=False)  # Wq\n",
        "\t\tself.tovalues  = nn.Linear(emb_size, emb_size * heads, bias=False)  # Wv\n",
        "\n",
        "\t\t# преобразование heads эмбеддингов обратно в 1 эмбеддинг\n",
        "\t\tself.lin = nn.Linear(heads * emb_size, emb_size)\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tОбработать последовательность\n",
        "\n",
        "\t\tx - тензор с эмбеддингами токенов вида (n_samples, embedding_size)\n",
        "\n",
        "\t\tвозвращает обработанный тензор того же вида\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tb, t, k = x.size()\n",
        "\t\th = self.heads\n",
        "\n",
        "\t\t# считаем матрицы Q, K и V\n",
        "\t\tqueries = self.toqueries(x).view(b, t, h, k)\n",
        "\t\tkeys    = self.tokeys(x).view(b, t, h, k)\n",
        "\t\tvalues  = self.tovalues(x).view(b, t, h, k)\n",
        "\n",
        "\t\tqueries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "\t\tkeys    = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "\t\tvalues  = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
        "\n",
        "\t\t# считаем веса каждого токена для друг друга\n",
        "\t\tdot = torch.bmm(queries, keys.transpose(1, 2)) / (k ** (1/4))\n",
        "\t\tdot = F.softmax(dot, dim=2)\n",
        "\n",
        "\t\t# считаем взвешанную сумму\n",
        "\t\tout = torch.bmm(dot, values).view(b, h, t, k)\n",
        "\t\tout = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
        "\n",
        "\t\t# преобразуем обратно в исходную размерность\n",
        "\t\tout = self.lin(out)\n",
        "\t\treturn out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRq44v2R839s"
      },
      "source": [
        "Имея на руках реализацию механизма внимания можно написать блок трансформера. Он аналогичен енкодеру из статьи `attention is all you need`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oMZVjiQ5c_Zf"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\t\"\"\"\n",
        "\tБлок трансформера\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, heads: int, embed_size: int, hiden_size: int):\n",
        "\t\t\"\"\"\n",
        "\t\theads - количество голов (видов связей) для self attention\n",
        "\t\tembed_size - размер эмбеддингов токенов\n",
        "\t\thiden_size - размер скрытого слоя полносвязной части блока\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.hiden_size = hiden_size\n",
        "\n",
        "\t\tself.norm1 = nn.LayerNorm(embed_size)   # нормализация после self attention\n",
        "\t\tself.attention = MultiHeadAttention(\n",
        "\t\t\tembed_size,\n",
        "\t\t\theads,\n",
        "\t\t)\n",
        "\t\tself.norm2 = nn.LayerNorm(embed_size)   # нормализация после полносвязной части\n",
        "\n",
        "\t\tself.mlp = nn.Sequential(\n",
        "\t\t\tnn.Linear(embed_size, hiden_size),\n",
        "\t\t\tnn.GELU(),\n",
        "\t\t\tnn.LayerNorm(hiden_size),\n",
        "\t\t\tnn.Linear(hiden_size, embed_size),\n",
        "\t\t)\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tОбработка последовательности\n",
        "\n",
        "\t\tx - тензор с эмбеддингами токенов вида (n_samples, embedding_size)\n",
        "\n",
        "\t\tвозвращает обработанный тензор того же вида\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tx = self.attention(x) + x\n",
        "\t\tx = self.norm1(x)\n",
        "\n",
        "\t\tx1 = self.mlp(x)\n",
        "\t\tx = x + x1\n",
        "\t\tx = self.norm2(x)\n",
        "\n",
        "\t\treturn x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyGYJoWg-M_g"
      },
      "source": [
        "У нас есть все части трансформера, напишем класс самой модели. В нем мы, вместо использования специального токена для классификации, полсе прохождения блоков трансформера делаем `Global Avarage Polling` всех токенов, как предложено в `Conditional Positional Encodings for Vision Transformers`, и затем по получившемуся среднему эмбеддингу делаем предсказание."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XyUxLxGec_Zg"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "\t\"\"\"\n",
        "\tВизуальный трансформер\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, blocks: int, heads: int, embed_size: int, hiden_size: int, patch_size: int, kernel_size: int):\n",
        "\t\t\"\"\"\n",
        "\t\tblocks - количество блоков рансформара\n",
        "\t\tembed_size - размер эмбеддинга токенов изображения\n",
        "\t\thiden_size - размер скрытого слоя полносвязной части в блоке транформера\n",
        "\t\tpatch_size - размер патча в пикселях\n",
        "\t\tkernel_size - размер куба, в котором рассматриваются ближайшие соседи, должен быть нечетным\n",
        "\t\t\"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.tokenizer = Tokenizer(embed_size, patch_size)\n",
        "\t\tself.pos_encoding = PositionalEncodingGenerator(embed_size, kernel_size)\n",
        "\n",
        "\t\tself.first_block = TransformerBlock(\n",
        "\t\t\t\t\t\t\t\theads,\n",
        "\t\t\t\t\t\t\t\tembed_size,\n",
        "\t\t\t\t\t\t\t\thiden_size\n",
        "\t\t\t\t\t\t\t)\n",
        "\n",
        "\t\tself.blocks = [\n",
        "\t\t\tTransformerBlock(\n",
        "\t\t\t\theads,\n",
        "\t\t\t\tembed_size,\n",
        "\t\t\t\thiden_size\n",
        "\t\t\t)\n",
        "\t\t\tfor _ in range(blocks - 1)\n",
        "\t\t]\n",
        "\n",
        "\t\t# нормализация и линейный слой для предсказания класса\n",
        "\t\tself.norm = nn.LayerNorm(embed_size)\n",
        "\t\tself.head = nn.Linear(embed_size, 1)\n",
        "\n",
        "\t\tself.sigmoid = nn.Sigmoid()\n",
        "\t\tself.loss = nn.BCELoss()\n",
        "\n",
        "\n",
        "\t# переопределяем метод to, так как стандартный не переводит блоки и скрытые ембеддинги на device\n",
        "\tdef to(self, device):\n",
        "\t\tfor block in self.blocks:\n",
        "\t\t\tblock.to(device)\n",
        "\n",
        "\t\treturn super().to(device)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tпрямой проход модели\n",
        "\n",
        "\t\tx - тензор картинок (тоже в виде тензора) размерностью (n_samples, n_channels, h_pixels, w_pixels)\n",
        "\n",
        "\t\tвозвращает тензор с предсказаниями\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# превращаем картинку в последовательность токенов\n",
        "\t\tx = self.tokenizer(x)\n",
        "\n",
        "\t\t# раскладываем токены в ряд и пропускаем через первый блок\n",
        "\t\tbatch_size, h, w, embed_size = x.shape\n",
        "\t\tx = x.reshape(batch_size, h * w, embed_size)\n",
        "\t\tx = self.first_block(x)\n",
        "\n",
        "\t\t# складываем токены обратно в матрицу и добавляем positional encoding\n",
        "\t\tx = x.reshape(batch_size, h, w, embed_size)\n",
        "\t\tx = self.pos_encoding(x)\n",
        "\n",
        "\t\t# раскладываем токены в ряд\n",
        "\t\tx = x.reshape(batch_size, h * w, embed_size)\n",
        "\n",
        "\t\t# проходим блоки\n",
        "\t\tfor block in self.blocks:\n",
        "\t\t\tx = block(x)\n",
        "\n",
        "\t\t# делаем GAP и считаем предсказание\n",
        "\t\tx = self.head(x.mean(dim=1))\n",
        "\n",
        "\t\treturn self.sigmoid(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiNJufmQ_4Hh"
      },
      "source": [
        " ## 2. Подготовка датасета\n",
        "\n",
        " Выше было сказано, что класс `Tokenizer` предполагает, что ему на вход подается картинка, размеры которой кратны размеру патча, то есть всю картинку можно разбить на непересекающиеся патчи без потери информации. Конечно, на деле далеко не всегда размер картинки будет такой, так что перед токенизацией картинки мы увеличим ее размер, добавив черные рамки справа и снизу, чтобы ее размеры удовлетворяли условию выше. Напишем функцию для изменения размера изображения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "T60m5mRMc_Zh"
      },
      "outputs": [],
      "source": [
        "def pad_img(image: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Функция добавляет черные рамки справа и снизу, чтобы картинку можно было разбить на патчи\n",
        "\n",
        "    аргументы:\n",
        "      image - картинка в виде тензора размером (n_channels, h_pixels, w_pixels)\n",
        "      patch_size - размер патча в пикселях\n",
        "\n",
        "    возвращает картинку в виде тензора размером (n_channels, h_pixels, w_pixels)\n",
        "    \"\"\"\n",
        "\n",
        "    channels, height, width = image.shape\n",
        "\n",
        "    # Вычисляем количество патчей по каждой координате\n",
        "    num_patches_h = math.ceil(height / patch_size)\n",
        "    num_patches_w = math.ceil(width / patch_size)\n",
        "\n",
        "    # Вычисляем необходимый паддинг\n",
        "    padded_height = num_patches_h * patch_size\n",
        "    padded_width = num_patches_w * patch_size\n",
        "\n",
        "    # Создаем новый тензор с паддингом\n",
        "    padded_image = torch.zeros(channels, padded_height, padded_width, device=image.device)\n",
        "    padded_image[:, :height, :width] = image\n",
        "\n",
        "    return padded_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HZdynucCohL"
      },
      "source": [
        "Определим класс датасета и заложим в него необходимые преобразования картинки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DOiUattcc_Zi"
      },
      "outputs": [],
      "source": [
        "class CatsDogsDataSet(Dataset):\n",
        "\t\"\"\"\n",
        "\tКласс датасета картинок кошек и собак\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, path: str, patch_size = 32):\n",
        "\t\t\"\"\"\n",
        "\t\tpath - путь к датасету\n",
        "\t\tpatch_size - размер патча\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.patch_size = patch_size\n",
        "\t\tself.path = path\n",
        "\n",
        "\t\tself.data_list = []     # список пар (путь к картинке, класс картинки)\n",
        "\n",
        "\t\t# перебираем папки и файлы по указанному пути\n",
        "\t\tfor folder in os.walk(path):\n",
        "\n",
        "\t\t\tfor file in folder[2]:\n",
        "\n",
        "\t\t\t\t# файл _DS_Store пропускаем\n",
        "\t\t\t\tif file == \"_DS_Store\":\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t# добавляем файл и его класс в список\n",
        "\t\t\t\tself.data_list.append((folder[0] + \"/\" + file, torch.tensor([1.0]) if folder[0][-2] == \"t\" else torch.tensor([0.0])))\n",
        "\n",
        "\t\tself.to_tensor = v2.Compose(\n",
        "\t\t\t[\n",
        "\t\t\t\tv2.ToTensor(),\n",
        "\t\t\t\tv2.ToDtype(torch.float32, scale=True),\n",
        "\t\t\t]\n",
        "\t\t)\n",
        "\t\tself.norm = v2.Normalize(mean=(0.5,), std=(0.5,))\n",
        "\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data_list)\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, index: int):\n",
        "\t\t'''\n",
        "\t\tindex - индекс семпла датасета\n",
        "\t\t'''\n",
        "\n",
        "\t\t# открываем картинку\n",
        "\t\tfile_path, target = self.data_list[index]\n",
        "\t\tsample = Image.open(file_path)\n",
        "\n",
        "\t\t# преобразуем в тензор, добавляем рамки и нормализуем\n",
        "\t\tsample = self.to_tensor(sample)\n",
        "\t\tsample = pad_img(sample, self.patch_size)\n",
        "\t\tsample = self.norm(sample)\n",
        "\n",
        "\t\treturn sample, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1t8766xSyzj"
      },
      "source": [
        "Создаем объекты класса датасета для тренеровочной и валидационной выборок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCfaHs4Dc_Zm",
        "outputId": "7b4d9620-41d8-4169-d481-ceda785ad808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_data = CatsDogsDataSet(path + \"/training_set/training_set\", 32)\n",
        "test_data = CatsDogsDataSet(path  + \"/test_set/test_set\", 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p6oaY81S79v"
      },
      "source": [
        "Создаем лоадеры для загрузки картинок по батчам. Размер батча равен единице, так как у картинок разный размер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qNbisRJCc_Zn"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sBjjzn-TNVK"
      },
      "source": [
        "## 3. Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qb5j_Updc_Zk",
        "outputId": "7bf7309a-fb6b-4625-a270-6efb7a3758bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "device =\"cuda\" if torch.cuda.is_available() else \"gpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gz3Ud4PTTy2"
      },
      "source": [
        "Создаем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EWWno3rbShNX"
      },
      "outputs": [],
      "source": [
        "model = VisionTransformer(\n",
        "    blocks=8,\n",
        "    heads=16,\n",
        "    embed_size=768,\n",
        "    hiden_size=1024,\n",
        "    patch_size=32,\n",
        "    kernel_size=5\n",
        ")\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AONtUVLwTYtc"
      },
      "source": [
        "Обучаем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4BB0Mb2c_Zn",
        "outputId": "7396322c-b231-4c71-e758-9b6e597096ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, train loss 0.7125, val loss 0.6950, train accuracy 0.5283, val accuracy 0.5329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, train loss 0.7094, val loss 0.7108, train accuracy 0.5332, val accuracy 0.5245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, train loss 0.7099, val loss 0.6915, train accuracy 0.5339, val accuracy 0.5487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, train loss 0.7122, val loss 0.6983, train accuracy 0.5328, val accuracy 0.5437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, train loss 0.7089, val loss 0.6967, train accuracy 0.5408, val accuracy 0.5630\n"
          ]
        }
      ],
      "source": [
        "# Оптимизатор Adam\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "epochs = 5        # количество эпох\n",
        "\n",
        "train_acc = []    # массив accuracy на тренеровочной выборке по эпохам\n",
        "val_acc = []      # массив accuracy на валидационной выборке по эпохам\n",
        "\n",
        "train_loss = []   # массив лоссов на тренеровочной выборке по эпохам\n",
        "val_loss = []     # массив лоссов на валидационной выборке по эпохам\n",
        "\n",
        "best_acc = 0      # лучший accuracy на валидационной выборке\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "\tmodel.train()\n",
        "\n",
        "\ttrain_loop = tqdm(train_loader, leave=False)\n",
        "\n",
        "\tcur_acc = 0\n",
        "\ttrain_loss_list = []\n",
        "\n",
        "\tfor sample, target in train_loop:\n",
        "\n",
        "\t\tsample = sample.to(device)\n",
        "\t\ttarget = target.to(device)\n",
        "\n",
        "\t\tpred = model(sample)\n",
        "\t\tloss = model.loss(pred, target)\n",
        "\t\ttrain_loss_list.append(loss.item())\n",
        "\n",
        "\t\topt.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\topt.step()\n",
        "\n",
        "\t\tcur_acc += int(int(pred.item() >= 0.5) == target.item())\n",
        "\t\tmean_train_loss = sum(train_loss_list) / len(train_loss_list)\n",
        "\n",
        "\t\ttrain_loop.set_description(f'Epoch {epoch + 1}/{epochs}, train_loss {mean_train_loss:.4f}')\n",
        "\n",
        "\ttrain_acc.append(cur_acc / (len(train_loader) * train_loader.batch_size))\n",
        "\ttrain_loss.append(mean_train_loss)\n",
        "\n",
        "\tmodel.eval()\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\n",
        "\t\tcur_acc = 0\n",
        "\t\tval_loss_list = []\n",
        "\n",
        "\t\tfor sample, target in test_loader:\n",
        "\n",
        "\t\t\tsample = sample.to(device)\n",
        "\t\t\ttarget = target.to(device)\n",
        "\n",
        "\t\t\tpred = model(sample)\n",
        "\t\t\tloss = model.loss(pred, target)\n",
        "\t\t\tval_loss_list.append(loss.item())\n",
        "\n",
        "\t\t\tcur_acc += int(int(pred.item() >= 0.5) == target.item())\n",
        "\t\t\tmean_val_loss = sum(val_loss_list) / len(val_loss_list)\n",
        "\n",
        "\tval_loss.append(mean_val_loss)\n",
        "\tval_acc.append(cur_acc / (len(test_loader) * test_loader.batch_size))\n",
        "\n",
        "\tif val_acc[-1] > best_acc:\n",
        "\t\tfilename = f\"model_acc_{val_acc[-1]:.3f}\".replace(\".\", \"\") + \".pt\"\n",
        "\t\ttorch.save(model.state_dict(), filename)\n",
        "\t\tdrive_path = f\"/content/drive/MyDrive/{filename}\"\n",
        "\t\ttorch.save(model.state_dict(), drive_path)\n",
        "\n",
        "\t\tbest_acc = val_acc[-1]\n",
        "\n",
        "\tprint(f'Epoch {epoch + 1}/{epochs}, train loss {train_loss[-1]:.4f}, val loss {val_loss[-1]:.4f}, train accuracy {train_acc[-1]:.4f}, val accuracy {val_acc[-1]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предсказание класса по изображению"
      ],
      "metadata": {
        "id": "zYcJCHZUfVL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms.v2 as v2\n",
        "\n",
        "image_path = \"/content/drive/MyDrive/images/dog.png\"\n",
        "\n",
        "\n",
        "patch_size = 32\n",
        "\n",
        "\n",
        "to_tensor = v2.Compose([\n",
        "    v2.ToTensor(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "])\n",
        "norm = v2.Normalize(mean=(0.5,), std=(0.5,))\n",
        "\n",
        "img = Image.open(image_path).convert(\"RGB\")\n",
        "img_tensor = to_tensor(img)\n",
        "img_tensor = pad_img(img_tensor, patch_size)\n",
        "img_tensor = norm(img_tensor)\n",
        "\n",
        "# Добавляем batch размер и отправляем на устройство\n",
        "img_tensor = img_tensor.unsqueeze(0).to(device)  # (1, C, H, W)\n",
        "\n",
        "# Переводим модель в режим оценки\n",
        "model.eval()\n",
        "\n",
        "# Предсказание\n",
        "with torch.no_grad():\n",
        "    pred = model(img_tensor)\n",
        "    predicted_class = int(pred.item() >= 0.5)\n",
        "\n",
        "# Вывод результата\n",
        "print(f\"Предсказанный класс: {predicted_class} ({'кошка' if predicted_class == 1 else 'собака'})\")"
      ],
      "metadata": {
        "id": "xuLOpgyJcmKx",
        "outputId": "52a60ba3-cadd-4647-94ad-059d0358e6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанный класс: 0 (собака)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
